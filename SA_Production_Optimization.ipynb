{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microeconomic modelling - Cobb Douglas Production function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objective 1:** Gradient descent vs Newton's method - differences and applications\n",
    "https://towardsdatascience.com/multivariate-differential-calculus-and-optimization-part-1-5c6b84831b27 <br> https://www.coursera.org/lecture/differentiation-calculus/optimization-Ew7ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "- Differential calculus is a powerful tool to find the optimal solution of a task\n",
    "    - one of the best applications of derivatives is to solve class of problems called max-min - optimization\n",
    "- A task is what we call an objective function \n",
    "- the results could be a maximum if the objective function is to maximise revenues \n",
    "- it can also be a minimum if your objective is to min costs \n",
    "- **Cobb-Douglas production function**:\n",
    "    - used for maximising output that can be produced given some labour and capital values\n",
    "    - used to represent the technological relationship between two or more inputs and the amount of output that can be produced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**\n",
    "\n",
    "- First derivative\n",
    "    - A derivative (the slope of the function at each point, x) of a function should always be 0 if we are to find maximum or minimum\n",
    "    - Min and max are critical points of a function \n",
    " \n",
    "<img src=\"images/derivative_1.PNG\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second Derivative\n",
    "    - the critical points that are neither max or min are called inflection points given by the 2nd derivative\n",
    "    - inflection point is a point of a curve at which a change in the direction of curvature occurs\n",
    "    - classification of critical points is subtle - functions can be discontinous, not differentiable at a point, not defined - this is where the derivative is not defined and we use the 2nd derivative\n",
    "\n",
    "<img src=\"images/inflection.PNG\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly there are many approaches to solving minimization and maximization problems and these approaches may be specific to the kind of function $f(x)$ that we are trying to minimize or maximize. Here we will learn iterative methods to find $x$ that minimizes/maximizes $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent** <br> https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c <br> https://medium.com/quantyca/gradient-descent-in-deep-learning-b1077b89af81 <br> https://builtin.com/data-science/gradient-descent <br> https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/ <br> Now that we know all this we can easily say that Gradient descent/ascent is an iterative optimization algorithm for finding the local minimum/maximum of a function. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradient_descent_mountain.PNG\" width=\"400\" />\n",
    "\n",
    "- we are at the top of the mountain and trying to reach the lowest point\n",
    "- see where the ground descends so we can take a step in the descending direction \n",
    "- repeat the process until we reach the lowest point (global min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate**\n",
    "- Once we have the direction we want to move in, we must decide the size of the step we must take - learning rate. \n",
    "- too slow - long training that might not be sustainable\n",
    "- too fast - we might overshoot the min and keep bouncing - might not reach the min\n",
    "\n",
    "<img src=\"images/learning_rate.PNG\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison with neural networks**\n",
    "- same logic as in neural networks\n",
    "- Neural network training goal is to find the weights for which the loss is minimized (real and predicted targets)\n",
    "- choose random weights at first - corresponds to the top of the mountain where we check out all possible directions and we take the direction with the steepest decline. \n",
    "- update the weights in the function until the slope of the funtion at this particular point/weight is 0 or this is when the first derivative of the function at this point is 0. This is where we have the local extreme given the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton's method or Newton-Raphson method** <br> https://www.baeldung.com/cs/gradient-descent-vs-newtons-gradient- <br> https://medium.com/datadriveninvestor/part-7-review-of-gradients-hessians-and-newtons-method-with-examples-implemented-in-tensorflow-9a1798a4c33b <br> https://medium.com/@FreeOfConfines/part-5-introduction-to-gradient-descent-and-newtons-algorithms-with-tensorflow-769c61616dad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the method for finding the inflection points of a function rather than max or min\n",
    "- We apply the method on the derivative of a function rather than the function itself - 2nd derivative\n",
    "- This means that the cost function must be differentiable twice not just once\n",
    "- Newton's method requires the analytical form of the derivative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Newton.PNG\" width=\"200\" />\n",
    "\n",
    "- Newton's method uses curvature information (i.e. the second derivative) to take a more direct route - red line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differences** <br> https://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/14-newton.pdf <br> https://www.baeldung.com/cs/gradient-descent-vs-newtons-gradient-descent <br> https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning#:~:text=Gradient%20descent%20direction's%20cheaper%20to,Hessian%20on%20the%20first%20iteration. <br> https://www.quora.com/Is-Newtons-method-always-superior-to-gradient-descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gradient Descent is parametric according to the learning rate. Newton's method is not parametric, which means we can apply it without worrying about hyperparameter optimization. \n",
    "Exception: A parametric Newton also exists when we have a polynomial function with multiple roots. \n",
    "2. Computationally expensive - at each iteration solves n*n Hessian matrix - descibes the local curvature of a function of many variables while gradient - scaling or adding n-dimensional vectors\n",
    "    - A Hessian is a matrix of all the second partial and cross partial derivatives of a function\n",
    "3. Newton's method requires a lot of storage for n-dimensional equation it requires n*n storage\n",
    "4. Empirically more sensitive to bugs/numerical erros while gradient descent is more robust\n",
    "5. Newton's method has stronger constraints in terms of how differentible a function is. If the second derivative of a function is undefined then we can apply gradient descent but not newton's method.\n",
    "6. Gradient descent doesn't require the function to be differentiable twice - applies to a larger class of functions while Newton's method require the functions to be approximately quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**\n",
    "\n",
    "Gradient descent:\n",
    "- prediction of house price given the house size\n",
    "- maximising profits\n",
    "- minimise drag on an airplane wing\n",
    "- how do you find the best route through a complex network\n",
    "- widely used in ML where every algorithm is optimized through some optimization criteria - used to update the parameters of our model\n",
    "    - Parameters refer to coefficients in Linear Regression and weights in neural networks\n",
    "    \n",
    "Newton's method\n",
    "- Used in numerical analysis mostly\n",
    "- loss function in ML algorithms\n",
    "- aircraft flutter problems\n",
    "- vibration analysis\n",
    "- voice localization system\n",
    "- analysis of flow in water distribution networks\n",
    "- analysis of power flow in electric power systems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objective 2:** Optimizing production using Newton's method\n",
    "https://towardsdatascience.com/using-data-for-end-to-end-microeconomic-modeling-abe41031015b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**\n",
    "- quantity of output produced\n",
    "- quantity of capital input - raw materials used\n",
    "- quantity of labour input - how many workers needed\n",
    "- one observation represents one week of production\n",
    "\n",
    "**Question**: What K and L should be achieved to optimize the output given a budget constraint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request as url\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"https://neos-guide.org/sites/default/files/mizon_data.txt\"\n",
    "raw = [line.decode('utf-8').split() for line in url.urlopen(file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(raw[1:], columns=['week', 'output', 'capital', 'labor'])\n",
    "data.loc[71, 'labor'] = 102.240\n",
    "data = data.astype(float).astype(int)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regress to find the Cobb-Douglas Param** \n",
    "\n",
    "$Q = A\\cdot K^{\\alpha}\\cdot L^{\\beta}$ where ${\\beta} = 1-{\\alpha}$\n",
    "\n",
    "Find the relationship that K and L have with Q where ${\\alpha}$ is simply the percentage of capital used in the production process, whilst ${\\beta}$ is the percentage of labour used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that we simply perform log transformation on both sides of the equation and put our variables into a matrix form <br>\n",
    "$ \\ln (Q) = \\ln (A) + {\\alpha}\\ln K + {\\beta}\\ln L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.        , 5.52146092, 5.59842196],\n",
       "        [1.        , 4.61512052, 4.53259949],\n",
       "        [1.        , 4.14313473, 1.94591015],\n",
       "        [1.        , 4.8598124 , 2.83321334],\n",
       "        [1.        , 5.98393628, 5.30330491],\n",
       "        [1.        , 5.8230459 , 5.22035583],\n",
       "        [1.        , 3.68887945, 3.29583687],\n",
       "        [1.        , 4.15888308, 4.07753744],\n",
       "        [1.        , 6.30627529, 6.71538339],\n",
       "        [1.        , 3.73766962, 4.99043259]]),\n",
       " array([5.9348942 , 5.25749537, 3.29583687, 3.4339872 , 6.03308622,\n",
       "        5.77765232, 4.02535169, 4.67282883, 7.10002717, 5.1590553 ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_y = np.log(data['output'].values)\n",
    "ln_X = np.c_[\n",
    "    np.ones((data.shape[0],1)), \n",
    "    np.log(data[['capital', 'labor']].values),\n",
    "    ]\n",
    "ln_X[0:10], ln_y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  then solve for ùúÉ (the vector of parameters) using the closed form regression equation\n",
    "\n",
    "$ X\\cdot \\theta = y $, which solves to $ \\theta = (X^{T}X)^{-1}y$ where $ \\theta  = [A, {\\alpha}, {\\beta}] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7785664111444337, 0.2383784740107106, 0.7616215259892893)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.linalg.inv(ln_X.T.dot(ln_X)).dot(ln_X.T).dot(ln_y)\n",
    "\n",
    "e = (1-(theta[1]+theta[2]))/2\n",
    "\n",
    "theta_0 = np.exp(theta[0])\n",
    "theta_1 = theta[1]+e\n",
    "theta_2 = theta[2]+e\n",
    "\n",
    "theta_0, theta_1, theta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plug the values back into the main equation\n",
    "\n",
    "$ Q = 1.779K^{0.238}L^{0.762} $\n",
    "\n",
    "Budget Constraint: $ 5000 = 1.5K + 1.92L $\n",
    "\n",
    "Now we have the company's budget and the company's production function so we can find out what K and L the company should use to max its production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal quantities of capital and labor, we must find a point in the production function with the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solve the problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>output</th>\n",
       "      <th>capital</th>\n",
       "      <th>labor</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>378</td>\n",
       "      <td>250</td>\n",
       "      <td>270</td>\n",
       "      <td>471.483335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>101</td>\n",
       "      <td>93</td>\n",
       "      <td>168.692644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>21.020304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>129</td>\n",
       "      <td>17</td>\n",
       "      <td>49.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>417</td>\n",
       "      <td>397</td>\n",
       "      <td>201</td>\n",
       "      <td>420.464658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week  output  capital  labor       y_hat\n",
       "0     1     378      250    270  471.483335\n",
       "1     2     192      101     93  168.692644\n",
       "2     3      27       63      7   21.020304\n",
       "3     4      31      129     17   49.014618\n",
       "4     5     417      397    201  420.464658"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formulate production function with regressed params\n",
    "output = lambda k,l: theta_0 * k**theta_1 * l**theta_2\n",
    "\n",
    "y_hat = np.array([output(*x) for x in data[['capital', 'labor']].values])\n",
    "\n",
    "data['y_hat'] = y_hat\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the budget constraint\n",
    "bdgt, rent, wage = 5000, 1.5, 1.92\n",
    "\n",
    "# wage is the wage rate per customer, rent is the rental rate of capital\n",
    "labor_constraint = lambda k: (bdgt - rent*k) / wage\n",
    "\n",
    "capital_constraint = lambda l: (bdgt - wage*l) / rent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move the budget constraint into the objective function. This new objective function is in Lagrange form. Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints \n",
    "\n",
    "$ L(x,{\\lambda}) = f(x) - {\\lambda}g(x) $\n",
    "\n",
    "$ 1.779K^{0.238}L^{0.762} + {\\lambda}(500 - 1.5K - 1.92L) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton's method implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method \n",
    "\n",
    "- In the case of a single variable, Newton‚Äôs method can be very straight forward\n",
    "- In the problem we are solving there is more than just 1 variable - K,L,lambda\n",
    "\n",
    "\n",
    "- In order to begin Newton‚Äôs method, we need an initial guess for what the true values may be\n",
    "    - set x to some value - in this case we are going to use the mean of K,L and Output\n",
    "- update the initial guess according to the direction of the steepest ascent\n",
    "    - the direction of the ascent is defined by the 1st derivative and the step size - by the 2nd\n",
    "    - calculate the function's gradient - a vector of all partial derivatives of a function\n",
    "- In order to take the 2nd differential - we have to calculate the Hessian\n",
    "    - A Hessian is a matrix of all the second partial and cross partial derivatives of a function\n",
    "- repeat the method a few times until you're satisfied that the method has converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bordered_hessian(a, k, l):\n",
    "    \n",
    "    (theta_0, theta_1, theta_2, rent, wage)\n",
    "    \n",
    "    L_kk = theta_0*theta_1*(theta_1-1) * k**(theta_1-2) * l**theta_2\n",
    "    L_ll = theta_0*theta_2*(theta_2-1) * k**theta_1 * l**(theta_2-2)\n",
    "    L_kl = L_lk = theta_0 * theta_1 * theta_2 * k**(theta_1-1) * l**(theta_2-1)\n",
    "    \n",
    "    return np.array([\n",
    "        [ 0,    -rent, -wage],\n",
    "        [-rent, L_kk,  L_kl ],\n",
    "        [-wage, L_lk,  L_ll ],\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(a, k, l):\n",
    "    \n",
    "    (theta_0, theta_1, theta_2, rent, wage, bdgt)\n",
    "    \n",
    "    L_a = bdgt - rent*k - wage*l\n",
    "    L_k = theta_0*theta_1 * k**(theta_1-1) * l**theta_2 - a*rent\n",
    "    L_l = theta_0*theta_2 * k**theta_1 * l**(theta_2-1) - a*wage\n",
    "\n",
    "    return np.array([\n",
    "        [L_a], [L_k], [L_l],\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton's Method**\n",
    "\n",
    "$ x = x_0 + f'(x)/f''(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0.4710968825, 1564.4570567694, 1381.9345910655]\n",
      "2 [0.5063262997, 508.3441931369, 2207.0227657785]\n",
      "3 [0.5532292225, 712.122289855, 2047.8211277175]\n",
      "4 [0.5662901015, 788.4360130891, 1988.2010314408]\n",
      "5 [0.5672901093, 794.5620012611, 1983.4151031814]\n",
      "6 [0.5672954618, 794.5949124325, 1983.3893913288]\n",
      "7 [0.5672954619, 794.594913369, 1983.3893905971]\n"
     ]
    }
   ],
   "source": [
    "# set x to some value - in this case we are going to use the mean of our variables\n",
    "targets = np.array([[ 1 ], [data['capital'].mean()], [data['labor'].mean()]])\n",
    "\n",
    "for i in np.arange(30):\n",
    "    \n",
    "    # set the initial guess to be the starting point\n",
    "    # previous = x_0\n",
    "    previous = targets    \n",
    "        \n",
    "    # the direction of the steepest ascent is the 1st derivative\n",
    "    g = gradient(*targets.flatten())\n",
    "    # Hessian m-x used in the second derivative\n",
    "    h = bordered_hessian(*targets.flatten())\n",
    "\n",
    "    # step size is the 2nd derivative\n",
    "    step = np.linalg.inv(h).dot(g)\n",
    "    # update the initial guess according to the direction of the steepest ascent and the step size\n",
    "    targets = targets - step\n",
    "    \n",
    "    # if x = x0 or when the initial guess = the local extreme we have reached \n",
    "    if round(np.sum(targets-previous), 10) == 0:\n",
    "        break\n",
    "    else:\n",
    "        print(i+1, [round(i, 10) for i in targets.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Capital: 794.5949133690358\n",
      "Optimal Labor 1983.3893905971076\n",
      "Optimal Production: 2836.4773097170837\n"
     ]
    }
   ],
   "source": [
    "# In the 7th iteration we have managed to reach a convergence\n",
    "print('Optimal Capital:', targets[1][0])\n",
    "print('Optimal Labor', targets[2][0]) \n",
    "print('Optimal Production:', output(*targets.flatten()[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solve for variables the easy way** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital: 794.5949133690355\n",
      "labor: 1983.3893905971076\n",
      "lambda: 0.0018877543447139485\n"
     ]
    }
   ],
   "source": [
    "true_maximum = ( (theta_1 * bdgt) / rent , (theta_2 * bdgt) / wage )\n",
    "true_lambda = rent / (bdgt / (rent + rent*theta_2 / theta_1))\n",
    "print('capital:', true_maximum[0])\n",
    "print('labor:', true_maximum[1])\n",
    "print('lambda:', true_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Capital: 794.5949133690355\n",
      "LaborL 1983.3893905971076\n",
      "Production: 2836.4773097170837\n"
     ]
    }
   ],
   "source": [
    "print('Optimal Capital:', true_maximum[0])\n",
    "print('LaborL', true_maximum[1]) \n",
    "print('Production:', output(true_maximum[0], true_maximum[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objective 2:** Pyomo\n",
    "\n",
    "https://www.ima.umn.edu/materials/2017-2018.2/W8.21-25.17/26326/3_PyomoFundamentals.pdf <br> https://colab.research.google.com/github/jckantor/CBE30338/blob/master/docs/06.04-Linear-Production-Model-in-Pyomo.ipynb#scrollTo=lDU-ku2aIzw6 <br> https://static1.squarespace.com/static/5492d7f4e4b00040889988bd/t/57bd0faad482e927298cca8f/1472008110099/5_Nonlinear.pdf <br> https://jckantor.github.io/CBE30338/06.99-Pyomo-Examples.html\n",
    "\n",
    "Pyomo is a Python-based open-source software package that supports a diverse set of optimization capabilities for formulating and analyzing optimization models.\n",
    "Pyomo supports a wide range of problem types, including:\n",
    "\n",
    "- Linear programming\n",
    "- Quadratic programming\n",
    "- **Nonlinear programming**\n",
    "- Mixed-integer linear programming\n",
    "- Mixed-integer quadratic programming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipopt --no-use-pep517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pyomo help --solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyomo.environ import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budget Constraint: 5000 = 1.5K + 1.92L\n",
    "\n",
    "Production Function: Q = 1.779K^0.238*L^0.762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantities of labor and capital the company should use in order to maximize its production**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLPSOL: GLPK LP/MIP Solver, v4.65\n",
      "Parameter(s) specified in the command line:\n",
      " --write C:\\Users\\astas\\AppData\\Local\\Temp\\tmptu2gkl17.glpk.raw --wglp C:\\Users\\astas\\AppData\\Local\\Temp\\tmpcdazncep.glpk.glp\n",
      " --cpxlp C:\\Users\\astas\\AppData\\Local\\Temp\\tmp7lzf4j23.pyomo.lp\n",
      "Reading problem data from 'C:\\Users\\astas\\AppData\\Local\\Temp\\tmp7lzf4j23.pyomo.lp'...\n",
      "2 rows, 3 columns, 3 non-zeros\n",
      "21 lines were read\n",
      "Writing problem data to 'C:\\Users\\astas\\AppData\\Local\\Temp\\tmpcdazncep.glpk.glp'...\n",
      "15 lines were written\n",
      "GLPK Simplex Optimizer, v4.65\n",
      "2 rows, 3 columns, 3 non-zeros\n",
      "Preprocessing...\n",
      "~     0: obj =   4.000000000e+04  infeas =  0.000e+00\n",
      "OPTIMAL SOLUTION FOUND BY LP PREPROCESSOR\n",
      "Time used:   0.0 secs\n",
      "Memory used: 0.0 Mb (32356 bytes)\n",
      "Writing basic solution to 'C:\\Users\\astas\\AppData\\Local\\Temp\\tmptu2gkl17.glpk.raw'...\n",
      "14 lines were written\n",
      "# ==========================================================\n",
      "# = Solver Results                                         =\n",
      "# ==========================================================\n",
      "# ----------------------------------------------------------\n",
      "#   Problem Information\n",
      "# ----------------------------------------------------------\n",
      "Problem: \n",
      "- Name: unknown\n",
      "  Lower bound: 40000.0\n",
      "  Upper bound: 40000.0\n",
      "  Number of objectives: 1\n",
      "  Number of constraints: 2\n",
      "  Number of variables: 3\n",
      "  Number of nonzeros: 3\n",
      "  Sense: maximize\n",
      "# ----------------------------------------------------------\n",
      "#   Solver Information\n",
      "# ----------------------------------------------------------\n",
      "Solver: \n",
      "- Status: ok\n",
      "  Termination condition: optimal\n",
      "  Statistics: \n",
      "    Branch and bound: \n",
      "      Number of bounded subproblems: 0\n",
      "      Number of created subproblems: 0\n",
      "  Error rc: 0\n",
      "  Time: 0.11601376533508301\n",
      "# ----------------------------------------------------------\n",
      "#   Solution Information\n",
      "# ----------------------------------------------------------\n",
      "Solution: \n",
      "- number of solutions: 0\n",
      "  number of solutions displayed: 0\n",
      "2 Var Declarations\n",
      "    K : Size=1, Index=None\n",
      "        Key  : Lower : Value : Upper : Fixed : Stale : Domain\n",
      "        None :     0 :   0.0 :  None : False : False : NonNegativeReals\n",
      "    L : Size=1, Index=None\n",
      "        Key  : Lower : Value : Upper : Fixed : Stale : Domain\n",
      "        None :     0 : 400.0 :  None : False : False : NonNegativeReals\n",
      "\n",
      "1 Objective Declarations\n",
      "    output : Size=1, Index=None, Active=True\n",
      "        Key  : Active : Sense    : Expression\n",
      "        None :   True : maximize : 50*K + 100*L\n",
      "\n",
      "1 Constraint Declarations\n",
      "    budget : Size=1, Index=None, Active=True\n",
      "        Key  : Lower  : Body               : Upper  : Active\n",
      "        None : 5000.0 : 50*K + 10*L + 1000 : 5000.0 :   True\n",
      "\n",
      "4 Declarations: K L output budget\n",
      "\n",
      "Optimized Capital =  0.0\n",
      "\n",
      "Optimized Labour = 400.0\n",
      "\n",
      "Optimized Output =  40000.0\n",
      "\n",
      "Budget Constraint =  5000.0\n"
     ]
    }
   ],
   "source": [
    "# create a model and store the model instance\n",
    "# AbstractModel() also exists\n",
    "model = ConcreteModel()\n",
    "\n",
    "# let's specify the decision variables so they are >= 0; we can have reals, integers, booleans, etc\n",
    "model.K = Var(domain=NonNegativeReals)\n",
    "model.L = Var(domain=NonNegativeReals)\n",
    "\n",
    "# set and store the objective \n",
    "#model.output = Objective(expr = 1.779*(model.K**0.238)*(model.L**0.762), sense=maximize)\n",
    "model.output = Objective(expr = (50*model.K)+(model.L*100), sense=maximize)\n",
    "\n",
    "# add the Constraint\n",
    "#model.budget = Constraint(expr = 1.5*model.K + 1.92*model.L == 5000)\n",
    "model.budget = Constraint(expr = 50*model.K + 10*model.L +1000 == 5000)\n",
    "\n",
    "# Use SolverFactory class to specify the solver\n",
    "results = SolverFactory('glpk').solve(model, tee=True)\n",
    "results.write()\n",
    "if results.solver.status:\n",
    "    model.pprint()\n",
    "    \n",
    "# print solutions\n",
    "capital = model.K()\n",
    "labour = model.L()\n",
    "output = model.output()\n",
    "budget = model.budget()\n",
    "print('\\nOptimized Capital = ', capital)\n",
    "print('\\nOptimized Labour =', labour)\n",
    "print('\\nOptimized Output = ', output)\n",
    "print('\\nBudget Constraint = ', budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model unknown\n",
      "\n",
      "  Variables:\n",
      "    K : Size=1, Index=None\n",
      "        Key  : Lower : Value : Upper : Fixed : Stale : Domain\n",
      "        None :     0 :   0.0 :  None : False : False : NonNegativeReals\n",
      "    L : Size=1, Index=None\n",
      "        Key  : Lower : Value : Upper : Fixed : Stale : Domain\n",
      "        None :     0 : 400.0 :  None : False : False : NonNegativeReals\n",
      "\n",
      "  Objectives:\n",
      "    output : Size=1, Index=None, Active=True\n",
      "        Key  : Active : Value\n",
      "        None :   True : 40000.0\n",
      "\n",
      "  Constraints:\n",
      "    budget : Size=1\n",
      "        Key  : Lower  : Body   : Upper\n",
      "        None : 5000.0 : 5000.0 : 5000.0\n"
     ]
    }
   ],
   "source": [
    "model.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:training]",
   "language": "python",
   "name": "conda-env-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
